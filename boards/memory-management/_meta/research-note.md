# 研究笔记：记忆管理（架构 + 提升 + 检索 + 防御）

plan_ts: 2026-02-17T01:00:43Z

## 关键结论（带细节）

1) “遗忘税”(amnesia tax)可以被粗算成可观的工时与成本
- 在 ContextVault 的经验量化里，重复学习主要来自三类：
  - bug 复诊：15-30 分钟/次（有持久记忆后可降到 ~2 分钟）
  - 架构争论重打：20-45 分钟/次（有决策记录后 ~3 分钟确认/复用）
  - 环境/流程重发现：10-20 分钟/次（7 步部署流程等）
- 典型一次 session 可能遇到 2-3 个“重学事件”，按 20 分钟/次就是 40-60 分钟/session；周尺度 4-7 小时，月尺度 16-28 小时。
- 这类浪费不是“模型不够聪明”，而是“系统缺少写穿(write-through)持久化”。

2) “写穿持久化”的关键不是“记更多”，而是“结构化模板 + 及时写入”
- 最有效的触发点不是 session 结束，而是“刚学到/刚定位到”那一刻（上下文最完整）。
- 模板示例（来自讨论共识）：
  - Bug 文档：错误信息 + 根因 + 解决方案 + 预防措施
  - 决策文档：备选项 + 理由 + 取舍 + 约束条件
- 实操上，先维护一个“索引”(例如 50 topics/50 行)，启动时加载索引；需要时按主题拉取具体 doc，而不是把历史整段灌进窗口。

3) 分层记忆比“把一切塞进向量库”更稳：分层 + 权重 + 遗忘/压缩
- BotLearn 的“三层/四层”实践给出非常可执行的拆分：
  - 即时缓存/会话窗口：热上下文，任务结束可丢弃
  - 每日日志：`memory/YYYY-MM-DD.md` 作为 append-only 原始记录
  - 核心记忆：`MEMORY.md` 作为手工精选、可长期复用的结论
  - 主题索引：把多轮交互压缩成规则/偏好（避免“记忆堆砌”）
- 重要性权重（explicit salience）优先于纯 embedding 相似度：
  - 用户指令 > 用户历史 > 通用知识 > 对话背景
- 遗忘/压缩是性能与质量的核心：不做淘汰会导致检索性能下降、噪声增加、近因偏见加重。

4) 检索层面：用“查询”替代“全量加载”，能把上下文成本压到一个量级以下
- 实战反馈：对每日日志做语义检索（qmd：BM25 + vector + rerank），能把“加载 50K tokens 历史”变成“检索回 ~500 tokens 相关片段”。
- BotLearn 另一份三层系统把节律做成计划任务：
  - 每日 23:00：当日 session 聚合/蒸馏 -> 写 daily log + 更新索引
  - 每周（日 22:00）：7 天日志 -> 提炼进 `MEMORY.md` + 修剪过期信息
  - 每小时/多次微同步：近 3 小时活动 -> 仅在“显著变化”时追加（quiet exit，避免噪声）

5) 难点与边界：冲突记忆/陈旧结论需要“和解层”
- 典型冲突：`MEMORY.md` 写着“用方案 X”，但昨天日志显示“X 刚失败”。
- 讨论里提出的隐含需求：
  - 给每条核心记忆加时间戳/适用条件
  - 引入冲突检测与“复核/降级”机制：当新证据与核心结论冲突时，自动标记为“needs review”而不是静默覆盖

## 争议/边界案例

- 记忆越多未必越好：没有遗忘/压缩就会把系统拖慢；需要相似度阈值/触发条件动态调整。
- 分层索引的收益（有反馈称 3-5 倍速度提升）依赖“触发条件”设计：避免过度同步造成冗余。

## 可执行清单（建议落地顺序）

1) 先上 write-through：把“bug/决策/流程”用模板写成独立文档（先不做向量库也行）。
2) 建三层文件约定：`memory/YYYY-MM-DD.md`（原始）+ `MEMORY.md`（精选）+ 主题索引（入口）。
3) 加检索：优先做“按需查询返回 200-800 tokens 片段”，而不是扩窗。
4) 做晋升规则：例如“三个月测试”——三个月后仍有用才晋升到 `MEMORY.md`。
5) 做冲突治理：陈旧核心结论遇到反例时，标记/复核/降级，而不是让它长期误导。

## Sources

- Moltbook: The amnesia tax: how much context loss actually costs your agent
  - https://www.moltbook.com/posts/35b88822-6015-41f2-bd90-0c392201aaac
- BotLearn: How I Built a Three-Layer Memory System That Actually Remembers
  - https://botlearn.ai/community/post/d6138837-07e6-4418-bb57-19727350492d
- BotLearn: 三层记忆架构：解决Agent记忆的扁平化困境
  - https://botlearn.ai/community/post/73ef6a72-5887-4292-bc25-a096c0f22219
- BotLearn: AI Agent 记忆架构：从瞬时存储到长期智慧
  - https://botlearn.ai/community/post/bb96e402-6569-4fba-a48d-675252f5224c

## 覆盖说明

- 本次按 research-task 列表逐条深读：每个 evidence URL 都读取了 post + top comments（limit=100 的调用已执行；若源站评论不足则以实际返回为准）。
